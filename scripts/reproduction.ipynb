{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of reproducing training results\n",
    "Prior to this step, please execute `download_weights.py` to download the weights for all pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from torch.utils.data import DataLoader\n",
    "from readData import readData\n",
    "import shutil\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import LABind\n",
    "from func_help import setALlSeed,get_std_opt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "DEVICE = torch.device('cuda:0')\n",
    "root_path = getRootPath()\n",
    "dataset = 'Unseen' # DS1:LigBind, DS2:GPSite, DS3:Unseen\n",
    "\n",
    "nn_config = {\n",
    "    # dataset \n",
    "    'train_file': f'{root_path}/{dataset}/label/training.fa',\n",
    "    'test_file': f'{root_path}/{dataset}/label/test.fa',\n",
    "    'valid_file': f'{root_path}/{dataset}/label/picking.fa',\n",
    "    'proj_dir': f'{root_path}/{dataset}/',\n",
    "    'lig_dict': pkl.load(open(f'{root_path}/tools/ligand.pkl', 'rb')),\n",
    "    'pdb_class':'source', # source or omegafold or esmfold\n",
    "    'dssp_max_repr': np.load(f'{root_path}/tools/dssp_max_repr.npy'),\n",
    "    'dssp_min_repr': np.load(f'{root_path}/tools/dssp_min_repr.npy'),\n",
    "    'ankh_max_repr': np.load(f'{root_path}/tools/ankh_max_repr.npy'),\n",
    "    'ankh_min_repr': np.load(f'{root_path}/tools/ankh_min_repr.npy'),\n",
    "    'ion_max_repr': np.load(f'{root_path}/tools/ion_max_repr.npy'),\n",
    "    'ion_min_repr': np.load(f'{root_path}/tools/ion_min_repr.npy'),\n",
    "    # model parameters\n",
    "    \n",
    "    'rfeat_dim':1556,\n",
    "    'ligand_dim':768, \n",
    "    'hidden_dim':256, \n",
    "    'heads':4, \n",
    "    'augment_eps':0.05, \n",
    "    'rbf_num':8, \n",
    "    'top_k':30, \n",
    "    'attn_drop':0.1, \n",
    "    'dropout':0.1, \n",
    "    'num_layers':4, \n",
    "    'lr':0.0004, \n",
    "    \n",
    "    # training parameters \n",
    "    # You can modify it according to the actual situation. \n",
    "    # Since it involves mapping the entire protein, it will consume a large amount of GPU memory.\n",
    "    'batch_size':15,\n",
    "    'max_patience':10,\n",
    "    'device_ids':[0,1], # 2*A100-40G\n",
    "}\n",
    "pretrain_path = { # Please modify \n",
    "    'esmfold_path': '../tools/esmfold_v1', # esmfold path\n",
    "    'ankh_path': '../tools/ankh-large/', # ankh path\n",
    "    'molformer_path': '../tools/MoLFormer-XL-both-10pct/', # molformer path\n",
    "    'model_path':f'{root_path}/model/Unseen/' # based on Unseen\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "Download the file from https://zenodo.org/records/13938443 and place it in the root directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ankh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ankh features\n",
    "from transformers import AutoTokenizer, T5EncoderModel \n",
    "from Bio import SeqIO\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['ankh_path'])\n",
    "model     = T5EncoderModel.from_pretrained(pretrain_path['ankh_path'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "out_path = f\"{root_path}/{dataset}/ankh/\"\n",
    "makeDir(out_path)\n",
    "# 使用biopython读取fasta文件\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    for fasta_base_file in os.listdir(f\"{fasta_path}/{file_class}\"):\n",
    "        fasta_file = f\"{fasta_path}/{file_class}/{fasta_base_file}\"\n",
    "        sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "        for record in tqdm(sequences):\n",
    "            if os.path.exists(out_path+f'{record.id}.npy'):\n",
    "                continue\n",
    "            ids = tokenizer.batch_encode_plus([list(record.seq)], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "            input_ids = ids['input_ids'].to(DEVICE)\n",
    "            attention_mask = ids['attention_mask'].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                emb = embedding_repr.last_hidden_state[0,:len(record.seq)].cpu().numpy()\n",
    "                np.save(out_path+f'{record.id}.npy',emb)\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "\n",
    "mapSS = {' ':[0,0,0,0,0,0,0,0,0],\n",
    "        '-':[1,0,0,0,0,0,0,0,0],\n",
    "        'H':[0,1,0,0,0,0,0,0,0],\n",
    "        'B':[0,0,1,0,0,0,0,0,0],\n",
    "        'E':[0,0,0,1,0,0,0,0,0],\n",
    "        'G':[0,0,0,0,1,0,0,0,0],\n",
    "        'I':[0,0,0,0,0,1,0,0,0],\n",
    "        'P':[0,0,0,0,0,0,1,0,0],\n",
    "        'T':[0,0,0,0,0,0,0,1,0],\n",
    "        'S':[0,0,0,0,0,0,0,0,1]}\n",
    "p = PDBParser(QUIET=True)\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "dssp_path = \"../tools/mkdssp\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_dssp/\")\n",
    "for pdb_file_name in tqdm(os.listdir(pdb_path),desc='DSSP running',ncols=80,unit='proteins'):\n",
    "    pdb_file = pdb_path+pdb_file_name\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_dssp')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    structure = p.get_structure(\"tmp\", pdb_file)\n",
    "    model = structure[0]\n",
    "    try:\n",
    "        dssp = DSSP(model, pdb_file, dssp=dssp_path)\n",
    "        keys = list(dssp.keys())\n",
    "    except:\n",
    "        keys = []\n",
    "    res_np = []\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            res_key = (chain.id,(' ', residue.id[1], residue.id[2]))\n",
    "            if res_key in keys:\n",
    "                tuple_dssp = dssp[res_key]\n",
    "                res_np.append(mapSS[tuple_dssp[2]] + list(tuple_dssp[3:]))\n",
    "            else:\n",
    "                res_np.append(np.zeros(20))\n",
    "    np.save(save_file, np.array(res_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.ResidueDepth import get_surface\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "msms_path = \"../tools/msms\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_pos/\")\n",
    "for pdb_file_name in tqdm(os.listdir(pdb_path),desc='MSMS running',ncols=80,unit='proteins'):\n",
    "    pdb_file = pdb_path+pdb_file_name\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_pos')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    X = []\n",
    "    chain_atom = ['N', 'CA', 'C', 'O']\n",
    "    model = parser.get_structure('model', pdb_file)[0]\n",
    "    chain = next(model.get_chains())\n",
    "    try:\n",
    "        surf = get_surface(chain,MSMS=msms_path)\n",
    "        surf_tree = cKDTree(surf)\n",
    "    except:\n",
    "        surf = np.empty(0)\n",
    "    for residue in chain:\n",
    "        line = []\n",
    "        atoms_coord = np.array([atom.get_coord() for atom in residue])\n",
    "        if surf.size == 0:\n",
    "            dist, _ = surf_tree.query(atoms_coord)\n",
    "            closest_atom = np.argmin(dist)\n",
    "            closest_pos = atoms_coord[closest_atom]\n",
    "        else:\n",
    "            closest_pos = atoms_coord[-1]\n",
    "        atoms = list(residue.get_atoms())\n",
    "        ca_pos= residue['CA'].get_coord()\n",
    "        pos_s = 0\n",
    "        un_s = 0\n",
    "        for atom in atoms:\n",
    "            if atom.name in chain_atom:\n",
    "                line.append(atom.get_coord())\n",
    "            else:\n",
    "                pos_s += calMass(atom,True)\n",
    "                un_s += calMass(atom,False)\n",
    "        # 此处line应该等于4\n",
    "        if len(line) != 4:\n",
    "            line = line + [list(ca_pos)]*(4-len(line))\n",
    "        if un_s == 0:\n",
    "            R_pos = ca_pos\n",
    "        else:\n",
    "            R_pos = pos_s / un_s\n",
    "        line.append(R_pos)  \n",
    "        line.append(closest_pos) # 加入最近点的残基信息\n",
    "        X.append(line) \n",
    "    np.save(save_file, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_list):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    valid_data = readData(\n",
    "        name_list=valid_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file']) # If 5-fold cross-validation is not used, it needs to be changed to valid_file.\n",
    "    valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=valid_data.collate_fn, num_workers=5)\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            logits = model(rfeat, ligand, xyz,  mask).sigmoid() # [N]\n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "        # 通过aupr数值进行早停\n",
    "        aupr_value = average_precision_score(all_y_true, all_y_score)\n",
    "    return aupr_value\n",
    "\n",
    "def train(train_list,valid_list=None,model=None,epochs=50,fold_idx=None):\n",
    "    model.to(DEVICE)\n",
    "    train_data = readData(\n",
    "        name_list=train_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'])\n",
    "    train_loader = DataLoader(train_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=train_data.collate_fn, num_workers=5)\n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "    optimizer = get_std_opt(len(train_list),nn_config['batch_size'], model.parameters(), nn_config['hidden_dim'], nn_config['lr'])\n",
    "    v_max_aupr = 0\n",
    "    patience = 0\n",
    "    t_mccs = []\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        all_loss = 0\n",
    "        all_cnt = 0\n",
    "        model.train()\n",
    "        for rfeat, ligand, xyz,  mask, y_true in tqdm(train_loader):\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(rfeat, ligand, xyz, mask).sigmoid() # [N]\n",
    "            # 计算所有离子的loss\n",
    "            loss = loss_fn(logits, y_true) * mask\n",
    "            loss = loss.sum() / mask.sum()\n",
    "            all_loss += loss.item()\n",
    "            all_cnt += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(all_loss / all_cnt)\n",
    "        # 根据验证集的aupr进行早停\n",
    "        if valid_list is not None:\n",
    "            v_aupr = valid(model,valid_list,fold_idx)\n",
    "            t_mccs.append(v_aupr)\n",
    "            print(f'Epoch {epoch} Loss: {all_loss / all_cnt}', f'Epoch Valid {epoch} AUPR: {v_aupr}')\n",
    "            if v_aupr > v_max_aupr:\n",
    "                v_max_aupr = v_aupr\n",
    "                patience = 0\n",
    "                torch.save(model.state_dict(), f'{root_path}/Output/{dataset}/fold{fold_idx}.ckpt')\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= nn_config['max_patience']:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setALlSeed(11)\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 42)\n",
    "data_list = readDataList(f'{root_path}/{dataset}/label/train/train.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}_5fold/')\n",
    "fold_idx = 0\n",
    "for train_idx, valid_idx in kf.split(data_list):\n",
    "    train_list = [data_list[i] for i in train_idx]\n",
    "    valid_list = [data_list[j] for j in valid_idx]\n",
    "    model = LABind(\n",
    "    rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "    rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "    train(train_list,valid_list,model,epochs=70,fold_idx=fold_idx)\n",
    "    fold_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setALlSeed(11)\n",
    "# train-validation-test\n",
    "train_list = readDataList(f'{root_path}/{dataset}/label/training.fa',skew=1)\n",
    "valid_list = readDataList(f'{root_path}/{dataset}/label/picking.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}/')\n",
    "model = LABind(\n",
    "rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "train(train_list, valid_list, model, epochs=70, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best threshold for MCC based on the validation set.\n",
    "model_path = f'{root_path}/Output/{dataset}/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(1): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    \n",
    "valid_list = readDataList(f'{root_path}/{dataset}/label/picking.fa',skew=1)\n",
    "valid_data = readData(\n",
    "    name_list=valid_list, \n",
    "    proj_dir=nn_config['proj_dir'], \n",
    "    lig_dict=nn_config['lig_dict'],\n",
    "    true_file=f'{root_path}/{dataset}/label/picking.fa')\n",
    "# 打印长度\n",
    "valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'], collate_fn=valid_data.collate_fn)\n",
    "print(f'valid data length: {len(valid_data)}')\n",
    "all_y_score = []\n",
    "all_y_true = []\n",
    "with torch.no_grad():\n",
    "    for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "        tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "        tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "        rfeat, ligand, xyz, mask, y_true = tensors\n",
    "        \n",
    "        logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "        logits = torch.stack(logits,0).mean(0)\n",
    "        \n",
    "        logits = torch.masked_select(logits, mask==1)\n",
    "        y_true = torch.masked_select(y_true, mask==1)\n",
    "        all_y_score.extend(logits.cpu().detach().numpy())\n",
    "        all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "\n",
    "best_threshold,best_mcc,best_pred = getBestThreshold(all_y_true, all_y_score)\n",
    "appendText(f'{model_path}/Best_Threshold.txt',f'{best_threshold} {best_mcc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_path = f'{root_path}/Output/{dataset}/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(1): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "df = pd.DataFrame(columns=['ligand','Rec','SPE','Acc','Pre','F1','MCC','AUC','AUPR'])\n",
    "for ionic in os.listdir(f'{root_path}/{dataset}/label/test/'):\n",
    "    ionic = ionic.split('.')[0]\n",
    "    test_list = readDataList(f'{root_path}/{dataset}/label/test/{ionic}.fa',skew=1)\n",
    "    test_data = readData(\n",
    "        name_list=test_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=f'{root_path}/{dataset}/label/test/{ionic}.fa')\n",
    "    # 打印长度\n",
    "    test_loader = DataLoader(test_data, batch_size=nn_config['batch_size'], collate_fn=test_data.collate_fn)\n",
    "    print(f'{ionic} test data length: {len(test_data)}')\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in test_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            \n",
    "            logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "            logits = torch.stack(logits,0).mean(0)\n",
    "            \n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "    data_dict = calEval(all_y_true, all_y_score, best_th = best_threshold) # please set best threshold.\n",
    "    data_dict['ligand'] = ionic\n",
    "    df = pd.concat([df,pd.DataFrame(data_dict,index=[0])])\n",
    "df.to_csv(f'{model_path}test.csv',index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzjone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
